{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Meets Bags of Popcorn\n",
    "\n",
    "[Kaggle Chanllenge](https://www.kaggle.com/c/word2vec-nlp-tutorial)\n",
    "Use Google's Word2Vec for movie reviews\n",
    "\n",
    "Deadline: 2019/01/05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#import re\n",
    "#import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv( \"Data/Word2Vec_clean_labeledTrainData.csv\", index_col=0 )\n",
    "\n",
    "unlabeled_train = pd.read_csv( \"Data/Word2Vec_clean_unlabeledTrainData.csv\", index_col=0 )\n",
    "\n",
    "test = pd.read_csv( \"Data/Word2Vec_clean_testData.csv\", index_col=0 )\n",
    "\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews.\\n\" \n",
    "      % (train[\"review\"].size, \n",
    "         test[\"review\"].size, \n",
    "         unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300        # size of each word vector\n",
    "MAX_VOCAB_SIZE = 200000    # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 500  # max number of words in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "We vectorize the text corpus by turning each text into a **sequence of integers**. Each integer is the index of a token in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, char_level=False)\n",
    "\n",
    "keras_tokenizer.fit_on_texts(train['review'].tolist() + test['review'].tolist())\n",
    "word_index = keras_tokenizer.word_index\n",
    "\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Transform each text to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train['review'].tolist())\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test['review'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad to the same length.\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word embedding matrix\n",
    "word2vec_model = Word2Vec.load(\"features300-minwords40-context10\")\n",
    "\n",
    "# Choose the smaller number one as column length\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[index] = word2vec_model.wv.get_vector(word)\n",
    "        \n",
    "# Null word embeddings: words don't exist in the embedding matrix and are represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = train[\"sentiment\"][index_train].tolist()\n",
    "y_val = train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layer1(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        num_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn_layer1_model = cnn_layer1()\n",
    "cnn_layer1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"Results/Bag_of_Words_model_feature3k_bigram.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classified_correct(model, i, inside=False):\n",
    "    if inside:\n",
    "        true_positive = model[\"sentiment\"][i] == 0 and int(model[\"id\"][i].split('\"')[1].split(\"_\")[1]) <= 5\n",
    "        true_negative = model[\"sentiment\"][i] == 1 and int(model[\"id\"][i].split('\"')[1].split(\"_\")[1]) > 5\n",
    "    \n",
    "    else:\n",
    "        true_positive = model[\"sentiment\"][i] == 0 and int(model[\"id\"][i].split(\"_\")[1]) <= 5\n",
    "        true_negative = model[\"sentiment\"][i] == 1 and int(model[\"id\"][i].split(\"_\")[1]) > 5\n",
    "    \n",
    "    return true_positive or true_negative\n",
    "\n",
    "target = output\n",
    "correct = np.array([classified_correct(target, i, inside=True) for i in range(target.shape[0])])\n",
    "print(correct.sum() / target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
